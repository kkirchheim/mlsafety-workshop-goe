{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"MIG-GPU-8ab9a0c8-909c-3f13-97e6-7376d6d4a029/0/0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This requires torchtext\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "from pytorch_ood.dataset.txt import NewsGroup20, Reuters52, WMT16Sentences, Multi30k, WikiText2\n",
    "from pytorch_ood.model.gruclf import GRUClassifier\n",
    "from pytorch_ood.utils import ToUnknown, OODMetrics\n",
    "from pytorch_ood.detector import MaxSoftmax, EnergyBased\n",
    "import torch.nn.functional as F\n",
    "from pytorch_ood.loss import OutlierExposureLoss\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "n_epochs = 10\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# download datasets\n",
    "train_dataset = NewsGroup20(\"data/\", train=True, download=True)\n",
    "test_dataset = NewsGroup20(\"data/\", train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11293lines [00:00, 15757.50lines/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for text, _ in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_dataset)) # , max_tokens=10000# , specials=[\"<unk>\"]\n",
    "# vocab.set_default_index(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add padding, etc.\n",
    "def collate_batch(batch):\n",
    "    texts = [i[0] for i in batch]\n",
    "    labels = torch.tensor([i[1] for i in batch],  dtype=torch.int64)\n",
    "    t_lengths = torch.tensor([len(t) for t in texts])\n",
    "    max_t_length = torch.max(t_lengths)\n",
    "\n",
    "    padded = []\n",
    "    for text in texts:\n",
    "        t = torch.cat([torch.zeros(max_t_length-len(text), dtype=torch.long), text])\n",
    "        padded.append(t)\n",
    "    return torch.stack(padded,dim=0), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test(model, dataset, dataset_name):\n",
    "    test_loader = DataLoader(dataset, batch_size=256, shuffle=True, collate_fn=collate_batch)\n",
    "    metrics = OODMetrics()\n",
    "    metrics_energy = OODMetrics()\n",
    "    softmax = MaxSoftmax(model)\n",
    "    energy = EnergyBased(model)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for n, batch in enumerate(test_loader):\n",
    "            inputs, labels = batch\n",
    "\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "            metrics.update(softmax(inputs), labels)\n",
    "            metrics_energy.update(energy(inputs), labels)\n",
    "\n",
    "    d1 = metrics.compute()\n",
    "    d1.update({\"Method\": \"Softmax\", \"Dataset\": dataset_name})\n",
    "    \n",
    "    d2 = metrics_energy.compute()\n",
    "    d2.update({\"Method\": \"Energy\", \"Dataset\": dataset_name})\n",
    "    return [d1, d2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Loss: 0.19 Accuracy 0.00%\n",
      "Loss: 1.26 Accuracy 1.14%\n",
      "Loss: 1.64 Accuracy 1.79%\n",
      "Loss: 1.75 Accuracy 1.92%\n",
      "Loss: 1.82 Accuracy 1.91%\n",
      "Loss: 1.79 Accuracy 2.02%\n",
      "Loss: 1.84 Accuracy 1.90%\n",
      "Loss: 1.82 Accuracy 1.94%\n",
      "Loss: 1.84 Accuracy 2.01%\n",
      "Loss: 1.85 Accuracy 1.96%\n",
      "Loss: 1.83 Accuracy 1.98%\n",
      "Loss: 1.89 Accuracy 2.08%\n",
      "Loss: 1.83 Accuracy 2.04%\n",
      "Loss: 1.84 Accuracy 1.98%\n",
      "Loss: 1.84 Accuracy 2.02%\n",
      "Loss: 1.83 Accuracy 1.95%\n",
      "Loss: 1.85 Accuracy 1.98%\n",
      "Loss: 1.83 Accuracy 1.90%\n",
      "Loss: 1.83 Accuracy 1.95%\n",
      "Loss: 1.88 Accuracy 1.91%\n",
      "Loss: 1.84 Accuracy 1.97%\n",
      "Loss: 1.81 Accuracy 1.95%\n",
      "Loss: 1.82 Accuracy 1.98%\n",
      "Loss: 1.80 Accuracy 1.95%\n",
      "Loss: 1.81 Accuracy 1.95%\n",
      "Loss: 1.84 Accuracy 1.98%\n",
      "Loss: 1.82 Accuracy 2.11%\n",
      "Loss: 1.82 Accuracy 2.13%\n",
      "Loss: 1.81 Accuracy 2.16%\n",
      "Loss: 1.86 Accuracy 2.18%\n",
      "Loss: 1.86 Accuracy 2.22%\n",
      "Loss: 1.89 Accuracy 2.27%\n",
      "Loss: 1.84 Accuracy 2.28%\n",
      "Loss: 1.85 Accuracy 2.31%\n",
      "Loss: 1.86 Accuracy 2.36%\n",
      "Loss: 1.81 Accuracy 2.39%\n",
      "Loss: 1.80 Accuracy 2.38%\n",
      "Loss: 1.82 Accuracy 2.40%\n",
      "Loss: 1.79 Accuracy 2.37%\n",
      "Loss: 1.79 Accuracy 2.41%\n",
      "Loss: 1.86 Accuracy 2.39%\n",
      "Loss: 1.82 Accuracy 2.43%\n",
      "Loss: 1.82 Accuracy 2.46%\n",
      "Loss: 1.83 Accuracy 2.47%\n",
      "Loss: 1.79 Accuracy 2.46%\n",
      "Loss: 1.81 Accuracy 2.50%\n",
      "Loss: 1.84 Accuracy 2.52%\n",
      "Loss: 1.84 Accuracy 2.54%\n",
      "Loss: 1.79 Accuracy 2.53%\n",
      "Loss: 1.77 Accuracy 2.56%\n",
      "Loss: 1.76 Accuracy 2.61%\n",
      "Loss: 1.82 Accuracy 2.61%\n",
      "Loss: 1.83 Accuracy 2.64%\n",
      "Loss: 1.83 Accuracy 2.67%\n",
      "Loss: 1.85 Accuracy 2.72%\n",
      "Loss: 1.78 Accuracy 2.75%\n",
      "Loss: 1.80 Accuracy 2.72%\n",
      "Loss: 1.81 Accuracy 2.70%\n",
      "Loss: 1.79 Accuracy 2.73%\n",
      "Loss: 1.81 Accuracy 2.76%\n",
      "Loss: 1.80 Accuracy 2.78%\n",
      "Loss: 1.80 Accuracy 2.78%\n",
      "Loss: 1.80 Accuracy 2.82%\n",
      "Loss: 1.78 Accuracy 2.84%\n",
      "Loss: 1.80 Accuracy 2.83%\n",
      "Loss: 1.80 Accuracy 2.86%\n",
      "Loss: 1.79 Accuracy 2.88%\n",
      "Loss: 1.77 Accuracy 2.90%\n",
      "Loss: 1.78 Accuracy 2.90%\n",
      "Loss: 1.80 Accuracy 2.91%\n",
      "Loss: 1.85 Accuracy 2.92%\n",
      "Loss: 1.85 Accuracy 2.93%\n",
      "Loss: 1.83 Accuracy 2.96%\n",
      "Loss: 1.84 Accuracy 2.96%\n",
      "Loss: 1.81 Accuracy 2.96%\n",
      "Loss: 1.78 Accuracy 3.00%\n",
      "Loss: 1.79 Accuracy 3.02%\n",
      "Loss: 1.77 Accuracy 3.02%\n",
      "Loss: 1.76 Accuracy 3.01%\n",
      "Loss: 1.76 Accuracy 3.03%\n",
      "Loss: 1.76 Accuracy 3.04%\n",
      "Loss: 1.74 Accuracy 3.05%\n",
      "Loss: 1.74 Accuracy 3.07%\n",
      "Loss: 1.77 Accuracy 3.10%\n",
      "Loss: 1.75 Accuracy 3.12%\n",
      "Loss: 1.74 Accuracy 3.15%\n",
      "Loss: 1.80 Accuracy 3.14%\n",
      "Loss: 1.77 Accuracy 3.17%\n",
      "Loss: 1.75 Accuracy 3.17%\n",
      "Loss: 1.74 Accuracy 3.20%\n",
      "Loss: 1.77 Accuracy 3.21%\n",
      "Loss: 1.77 Accuracy 3.22%\n",
      "Loss: 1.74 Accuracy 3.25%\n",
      "Loss: 1.79 Accuracy 3.25%\n",
      "Loss: 1.73 Accuracy 3.26%\n",
      "Loss: 1.74 Accuracy 3.27%\n",
      "Loss: 1.73 Accuracy 3.28%\n",
      "Loss: 1.73 Accuracy 3.31%\n",
      "Loss: 1.70 Accuracy 3.34%\n",
      "Loss: 1.72 Accuracy 3.36%\n",
      "Loss: 1.77 Accuracy 3.41%\n",
      "Loss: 1.73 Accuracy 3.45%\n",
      "Loss: 1.74 Accuracy 3.47%\n",
      "Loss: 1.74 Accuracy 3.49%\n",
      "Loss: 1.73 Accuracy 3.49%\n",
      "Loss: 1.70 Accuracy 3.51%\n",
      "Loss: 1.72 Accuracy 3.52%\n",
      "Loss: 1.69 Accuracy 3.52%\n",
      "Loss: 1.74 Accuracy 3.54%\n",
      "Loss: 1.72 Accuracy 3.55%\n",
      "Loss: 1.71 Accuracy 3.56%\n",
      "Loss: 1.74 Accuracy 3.59%\n",
      "Loss: 1.73 Accuracy 3.62%\n",
      "Loss: 1.72 Accuracy 3.62%\n",
      "Loss: 1.70 Accuracy 3.65%\n",
      "Loss: 1.69 Accuracy 3.66%\n",
      "Loss: 1.67 Accuracy 3.70%\n",
      "Loss: 1.66 Accuracy 3.73%\n",
      "Loss: 1.65 Accuracy 3.77%\n",
      "Loss: 1.68 Accuracy 3.80%\n",
      "Loss: 1.68 Accuracy 3.83%\n",
      "Loss: 1.69 Accuracy 3.84%\n",
      "Loss: 1.66 Accuracy 3.87%\n",
      "Loss: 1.68 Accuracy 3.92%\n",
      "Loss: 1.67 Accuracy 3.94%\n",
      "Loss: 1.68 Accuracy 3.95%\n",
      "Loss: 1.70 Accuracy 3.96%\n",
      "Loss: 1.68 Accuracy 3.98%\n",
      "Loss: 1.68 Accuracy 4.01%\n",
      "Loss: 1.63 Accuracy 4.04%\n",
      "Loss: 1.67 Accuracy 4.05%\n",
      "Loss: 1.67 Accuracy 4.09%\n",
      "Loss: 1.67 Accuracy 4.11%\n",
      "Loss: 1.67 Accuracy 4.12%\n",
      "Loss: 1.68 Accuracy 4.15%\n",
      "Loss: 1.65 Accuracy 4.16%\n",
      "Loss: 1.64 Accuracy 4.18%\n",
      "Loss: 1.61 Accuracy 4.20%\n",
      "Loss: 1.65 Accuracy 4.22%\n",
      "Loss: 1.65 Accuracy 4.24%\n",
      "Loss: 1.63 Accuracy 4.25%\n",
      "Loss: 1.64 Accuracy 4.27%\n",
      "Loss: 1.63 Accuracy 4.29%\n",
      "Loss: 1.64 Accuracy 4.31%\n",
      "Loss: 1.65 Accuracy 4.33%\n",
      "Loss: 1.61 Accuracy 4.36%\n",
      "Loss: 1.62 Accuracy 4.37%\n",
      "Loss: 1.62 Accuracy 4.38%\n",
      "Loss: 1.62 Accuracy 4.40%\n",
      "Loss: 1.61 Accuracy 4.42%\n",
      "Loss: 1.60 Accuracy 4.44%\n",
      "Loss 1.60 Test Accuracy: 32.03%\n",
      "Epoch 1\n",
      "Loss: 0.16 Accuracy 9.38%\n",
      "Loss: 1.05 Accuracy 11.08%\n",
      "Loss: 1.37 Accuracy 10.12%\n",
      "Loss: 1.52 Accuracy 8.97%\n",
      "Loss: 1.53 Accuracy 9.68%\n",
      "Loss: 1.52 Accuracy 9.74%\n",
      "Loss: 1.54 Accuracy 9.38%\n",
      "Loss: 1.57 Accuracy 9.11%\n",
      "Loss: 1.58 Accuracy 9.26%\n",
      "Loss: 1.58 Accuracy 9.38%\n",
      "Loss: 1.56 Accuracy 9.62%\n",
      "Loss: 1.53 Accuracy 9.54%\n",
      "Loss: 1.55 Accuracy 9.56%\n",
      "Loss: 1.49 Accuracy 9.90%\n",
      "Loss: 1.52 Accuracy 10.02%\n",
      "Loss: 1.55 Accuracy 10.00%\n",
      "Loss: 1.58 Accuracy 9.92%\n",
      "Loss: 1.55 Accuracy 9.89%\n",
      "Loss: 1.54 Accuracy 9.82%\n",
      "Loss: 1.56 Accuracy 9.72%\n",
      "Loss: 1.57 Accuracy 9.61%\n",
      "Loss: 1.51 Accuracy 9.72%\n",
      "Loss: 1.51 Accuracy 9.76%\n",
      "Loss: 1.54 Accuracy 9.70%\n",
      "Loss: 1.59 Accuracy 9.62%\n",
      "Loss: 1.59 Accuracy 9.64%\n",
      "Loss: 1.55 Accuracy 9.60%\n",
      "Loss: 1.54 Accuracy 9.57%\n",
      "Loss: 1.56 Accuracy 9.53%\n",
      "Loss: 1.57 Accuracy 9.56%\n",
      "Loss: 1.57 Accuracy 9.60%\n",
      "Loss: 1.51 Accuracy 9.60%\n",
      "Loss: 1.50 Accuracy 9.70%\n",
      "Loss: 1.52 Accuracy 9.68%\n",
      "Loss: 1.50 Accuracy 9.78%\n",
      "Loss: 1.48 Accuracy 9.84%\n",
      "Loss: 1.55 Accuracy 9.85%\n",
      "Loss: 1.52 Accuracy 9.82%\n",
      "Loss: 1.51 Accuracy 9.84%\n",
      "Loss: 1.55 Accuracy 9.85%\n",
      "Loss: 1.54 Accuracy 9.85%\n",
      "Loss: 1.56 Accuracy 9.87%\n",
      "Loss: 1.53 Accuracy 9.92%\n",
      "Loss: 1.52 Accuracy 9.92%\n",
      "Loss: 1.51 Accuracy 9.94%\n",
      "Loss: 1.50 Accuracy 9.94%\n",
      "Loss: 1.49 Accuracy 9.96%\n",
      "Loss: 1.50 Accuracy 9.97%\n",
      "Loss: 1.52 Accuracy 9.99%\n",
      "Loss: 1.53 Accuracy 10.02%\n",
      "Loss: 1.49 Accuracy 10.05%\n",
      "Loss: 1.50 Accuracy 10.07%\n",
      "Loss: 1.50 Accuracy 10.09%\n",
      "Loss: 1.50 Accuracy 10.09%\n",
      "Loss: 1.53 Accuracy 10.08%\n",
      "Loss: 1.47 Accuracy 10.13%\n",
      "Loss: 1.52 Accuracy 10.09%\n",
      "Loss: 1.52 Accuracy 10.14%\n",
      "Loss: 1.51 Accuracy 10.18%\n",
      "Loss: 1.52 Accuracy 10.17%\n",
      "Loss: 1.50 Accuracy 10.20%\n",
      "Loss: 1.52 Accuracy 10.19%\n",
      "Loss: 1.52 Accuracy 10.21%\n",
      "Loss: 1.49 Accuracy 10.24%\n",
      "Loss: 1.50 Accuracy 10.22%\n",
      "Loss: 1.51 Accuracy 10.24%\n",
      "Loss: 1.53 Accuracy 10.24%\n",
      "Loss: 1.51 Accuracy 10.27%\n",
      "Loss: 1.56 Accuracy 10.29%\n",
      "Loss: 1.54 Accuracy 10.25%\n",
      "Loss: 1.55 Accuracy 10.23%\n",
      "Loss: 1.54 Accuracy 10.23%\n",
      "Loss: 1.50 Accuracy 10.23%\n",
      "Loss: 1.47 Accuracy 10.25%\n",
      "Loss: 1.48 Accuracy 10.29%\n",
      "Loss: 1.49 Accuracy 10.28%\n",
      "Loss: 1.48 Accuracy 10.32%\n",
      "Loss: 1.49 Accuracy 10.34%\n",
      "Loss: 1.52 Accuracy 10.32%\n",
      "Loss: 1.51 Accuracy 10.32%\n",
      "Loss: 1.48 Accuracy 10.37%\n",
      "Loss: 1.48 Accuracy 10.37%\n",
      "Loss: 1.51 Accuracy 10.39%\n",
      "Loss: 1.48 Accuracy 10.42%\n",
      "Loss: 1.45 Accuracy 10.45%\n",
      "Loss: 1.50 Accuracy 10.45%\n",
      "Loss: 1.49 Accuracy 10.45%\n",
      "Loss: 1.48 Accuracy 10.47%\n",
      "Loss: 1.48 Accuracy 10.52%\n",
      "Loss: 1.51 Accuracy 10.54%\n",
      "Loss: 1.50 Accuracy 10.50%\n",
      "Loss: 1.47 Accuracy 10.53%\n",
      "Loss: 1.46 Accuracy 10.53%\n",
      "Loss: 1.50 Accuracy 10.56%\n",
      "Loss: 1.46 Accuracy 10.62%\n",
      "Loss: 1.44 Accuracy 10.66%\n",
      "Loss: 1.45 Accuracy 10.68%\n",
      "Loss: 1.45 Accuracy 10.69%\n",
      "Loss: 1.45 Accuracy 10.71%\n",
      "Loss: 1.44 Accuracy 10.74%\n",
      "Loss: 1.47 Accuracy 10.76%\n",
      "Loss: 1.44 Accuracy 10.77%\n",
      "Loss: 1.43 Accuracy 10.80%\n",
      "Loss: 1.42 Accuracy 10.83%\n",
      "Loss: 1.47 Accuracy 10.83%\n",
      "Loss: 1.48 Accuracy 10.85%\n",
      "Loss: 1.50 Accuracy 10.85%\n",
      "Loss: 1.48 Accuracy 10.88%\n",
      "Loss: 1.49 Accuracy 10.89%\n",
      "Loss: 1.49 Accuracy 10.92%\n",
      "Loss: 1.42 Accuracy 10.95%\n",
      "Loss: 1.44 Accuracy 10.96%\n",
      "Loss: 1.47 Accuracy 10.95%\n",
      "Loss: 1.46 Accuracy 10.99%\n",
      "Loss: 1.45 Accuracy 11.01%\n",
      "Loss: 1.48 Accuracy 11.03%\n",
      "Loss: 1.45 Accuracy 11.05%\n",
      "Loss: 1.44 Accuracy 11.07%\n",
      "Loss: 1.44 Accuracy 11.10%\n",
      "Loss: 1.46 Accuracy 11.09%\n",
      "Loss: 1.42 Accuracy 11.14%\n",
      "Loss: 1.41 Accuracy 11.18%\n",
      "Loss: 1.45 Accuracy 11.21%\n",
      "Loss: 1.41 Accuracy 11.25%\n",
      "Loss: 1.40 Accuracy 11.27%\n",
      "Loss: 1.43 Accuracy 11.29%\n",
      "Loss: 1.45 Accuracy 11.31%\n",
      "Loss: 1.42 Accuracy 11.34%\n",
      "Loss: 1.43 Accuracy 11.39%\n",
      "Loss: 1.47 Accuracy 11.39%\n",
      "Loss: 1.46 Accuracy 11.39%\n",
      "Loss: 1.47 Accuracy 11.39%\n",
      "Loss: 1.48 Accuracy 11.41%\n",
      "Loss: 1.44 Accuracy 11.42%\n",
      "Loss: 1.42 Accuracy 11.44%\n",
      "Loss: 1.46 Accuracy 11.44%\n",
      "Loss: 1.48 Accuracy 11.43%\n",
      "Loss: 1.47 Accuracy 11.44%\n",
      "Loss: 1.44 Accuracy 11.46%\n",
      "Loss: 1.45 Accuracy 11.45%\n",
      "Loss: 1.45 Accuracy 11.47%\n",
      "Loss: 1.45 Accuracy 11.49%\n",
      "Loss: 1.45 Accuracy 11.50%\n",
      "Loss: 1.43 Accuracy 11.51%\n",
      "Loss: 1.41 Accuracy 11.52%\n",
      "Loss: 1.44 Accuracy 11.52%\n",
      "Loss: 1.42 Accuracy 11.53%\n",
      "Loss: 1.43 Accuracy 11.55%\n",
      "Loss: 1.46 Accuracy 11.56%\n",
      "Loss: 1.38 Accuracy 11.60%\n",
      "Loss: 1.44 Accuracy 11.63%\n",
      "Loss 1.44 Test Accuracy: 49.91%\n",
      "Epoch 2\n",
      "Loss: 0.14 Accuracy 9.38%\n",
      "Loss: 0.95 Accuracy 13.92%\n",
      "Loss: 1.20 Accuracy 15.18%\n",
      "Loss: 1.27 Accuracy 16.83%\n",
      "Loss: 1.33 Accuracy 16.77%\n",
      "Loss: 1.33 Accuracy 16.85%\n",
      "Loss: 1.32 Accuracy 16.96%\n",
      "Loss: 1.35 Accuracy 17.08%\n",
      "Loss: 1.35 Accuracy 17.01%\n",
      "Loss: 1.34 Accuracy 16.86%\n",
      "Loss: 1.34 Accuracy 16.65%\n",
      "Loss: 1.32 Accuracy 16.95%\n",
      "Loss: 1.35 Accuracy 16.66%\n",
      "Loss: 1.31 Accuracy 16.82%\n",
      "Loss: 1.34 Accuracy 16.60%\n",
      "Loss: 1.31 Accuracy 16.68%\n",
      "Loss: 1.30 Accuracy 16.75%\n",
      "Loss: 1.34 Accuracy 16.59%\n",
      "Loss: 1.40 Accuracy 16.26%\n",
      "Loss: 1.35 Accuracy 16.28%\n",
      "Loss: 1.36 Accuracy 16.32%\n",
      "Loss: 1.34 Accuracy 16.42%\n",
      "Loss: 1.36 Accuracy 16.36%\n",
      "Loss: 1.32 Accuracy 16.36%\n",
      "Loss: 1.32 Accuracy 16.48%\n",
      "Loss: 1.31 Accuracy 16.60%\n",
      "Loss: 1.30 Accuracy 16.64%\n",
      "Loss: 1.31 Accuracy 16.76%\n",
      "Loss: 1.37 Accuracy 16.60%\n",
      "Loss: 1.34 Accuracy 16.60%\n",
      "Loss: 1.34 Accuracy 16.55%\n",
      "Loss: 1.30 Accuracy 16.61%\n",
      "Loss: 1.35 Accuracy 16.53%\n",
      "Loss: 1.32 Accuracy 16.56%\n",
      "Loss: 1.33 Accuracy 16.48%\n",
      "Loss: 1.34 Accuracy 16.44%\n",
      "Loss: 1.33 Accuracy 16.53%\n",
      "Loss: 1.31 Accuracy 16.61%\n",
      "Loss: 1.32 Accuracy 16.65%\n",
      "Loss: 1.28 Accuracy 16.75%\n",
      "Loss: 1.34 Accuracy 16.72%\n",
      "Loss: 1.34 Accuracy 16.69%\n",
      "Loss: 1.32 Accuracy 16.76%\n",
      "Loss: 1.35 Accuracy 16.73%\n",
      "Loss: 1.33 Accuracy 16.73%\n",
      "Loss: 1.35 Accuracy 16.65%\n",
      "Loss: 1.35 Accuracy 16.61%\n",
      "Loss: 1.36 Accuracy 16.64%\n",
      "Loss: 1.36 Accuracy 16.60%\n",
      "Loss: 1.35 Accuracy 16.56%\n",
      "Loss: 1.35 Accuracy 16.55%\n",
      "Loss: 1.34 Accuracy 16.59%\n",
      "Loss: 1.32 Accuracy 16.63%\n",
      "Loss: 1.31 Accuracy 16.70%\n",
      "Loss: 1.31 Accuracy 16.71%\n",
      "Loss: 1.33 Accuracy 16.69%\n",
      "Loss: 1.26 Accuracy 16.79%\n",
      "Loss: 1.30 Accuracy 16.83%\n",
      "Loss: 1.32 Accuracy 16.81%\n",
      "Loss: 1.37 Accuracy 16.78%\n",
      "Loss: 1.33 Accuracy 16.83%\n",
      "Loss: 1.35 Accuracy 16.81%\n",
      "Loss: 1.35 Accuracy 16.77%\n",
      "Loss: 1.32 Accuracy 16.80%\n",
      "Loss: 1.35 Accuracy 16.76%\n",
      "Loss: 1.32 Accuracy 16.75%\n",
      "Loss: 1.37 Accuracy 16.71%\n",
      "Loss: 1.33 Accuracy 16.72%\n",
      "Loss: 1.30 Accuracy 16.75%\n",
      "Loss: 1.31 Accuracy 16.77%\n",
      "Loss: 1.33 Accuracy 16.77%\n",
      "Loss: 1.32 Accuracy 16.76%\n",
      "Loss: 1.30 Accuracy 16.78%\n",
      "Loss: 1.29 Accuracy 16.84%\n",
      "Loss: 1.29 Accuracy 16.87%\n",
      "Loss: 1.33 Accuracy 16.88%\n",
      "Loss: 1.30 Accuracy 16.91%\n",
      "Loss: 1.34 Accuracy 16.88%\n",
      "Loss: 1.33 Accuracy 16.87%\n",
      "Loss: 1.36 Accuracy 16.81%\n",
      "Loss: 1.37 Accuracy 16.76%\n",
      "Loss: 1.36 Accuracy 16.74%\n",
      "Loss: 1.32 Accuracy 16.73%\n",
      "Loss: 1.35 Accuracy 16.71%\n",
      "Loss: 1.32 Accuracy 16.71%\n",
      "Loss: 1.36 Accuracy 16.69%\n",
      "Loss: 1.37 Accuracy 16.63%\n",
      "Loss: 1.36 Accuracy 16.62%\n",
      "Loss: 1.33 Accuracy 16.62%\n",
      "Loss: 1.35 Accuracy 16.61%\n",
      "Loss: 1.33 Accuracy 16.65%\n",
      "Loss: 1.34 Accuracy 16.63%\n",
      "Loss: 1.31 Accuracy 16.63%\n",
      "Loss: 1.32 Accuracy 16.63%\n",
      "Loss: 1.33 Accuracy 16.63%\n",
      "Loss: 1.31 Accuracy 16.65%\n",
      "Loss: 1.29 Accuracy 16.68%\n",
      "Loss: 1.29 Accuracy 16.70%\n",
      "Loss: 1.31 Accuracy 16.71%\n",
      "Loss: 1.30 Accuracy 16.73%\n",
      "Loss: 1.32 Accuracy 16.72%\n",
      "Loss: 1.31 Accuracy 16.72%\n",
      "Loss: 1.34 Accuracy 16.73%\n",
      "Loss: 1.35 Accuracy 16.73%\n",
      "Loss: 1.37 Accuracy 16.73%\n",
      "Loss: 1.33 Accuracy 16.74%\n",
      "Loss: 1.37 Accuracy 16.71%\n",
      "Loss: 1.37 Accuracy 16.69%\n",
      "Loss: 1.31 Accuracy 16.73%\n",
      "Loss: 1.36 Accuracy 16.72%\n",
      "Loss: 1.36 Accuracy 16.74%\n",
      "Loss: 1.34 Accuracy 16.76%\n",
      "Loss: 1.35 Accuracy 16.73%\n",
      "Loss: 1.33 Accuracy 16.73%\n",
      "Loss: 1.34 Accuracy 16.71%\n",
      "Loss: 1.36 Accuracy 16.70%\n",
      "Loss: 1.33 Accuracy 16.72%\n",
      "Loss: 1.32 Accuracy 16.72%\n",
      "Loss: 1.32 Accuracy 16.74%\n",
      "Loss: 1.32 Accuracy 16.73%\n",
      "Loss: 1.24 Accuracy 16.79%\n",
      "Loss: 1.23 Accuracy 16.83%\n",
      "Loss: 1.29 Accuracy 16.85%\n",
      "Loss: 1.31 Accuracy 16.88%\n",
      "Loss: 1.32 Accuracy 16.86%\n",
      "Loss: 1.30 Accuracy 16.89%\n",
      "Loss: 1.32 Accuracy 16.89%\n",
      "Loss: 1.32 Accuracy 16.89%\n",
      "Loss: 1.31 Accuracy 16.91%\n",
      "Loss: 1.30 Accuracy 16.90%\n",
      "Loss: 1.30 Accuracy 16.90%\n",
      "Loss: 1.33 Accuracy 16.89%\n",
      "Loss: 1.36 Accuracy 16.86%\n",
      "Loss: 1.34 Accuracy 16.86%\n",
      "Loss: 1.31 Accuracy 16.88%\n",
      "Loss: 1.34 Accuracy 16.89%\n",
      "Loss: 1.32 Accuracy 16.88%\n",
      "Loss: 1.33 Accuracy 16.90%\n",
      "Loss: 1.35 Accuracy 16.90%\n",
      "Loss: 1.33 Accuracy 16.89%\n",
      "Loss: 1.34 Accuracy 16.87%\n",
      "Loss: 1.34 Accuracy 16.86%\n",
      "Loss: 1.30 Accuracy 16.87%\n",
      "Loss: 1.33 Accuracy 16.85%\n",
      "Loss: 1.34 Accuracy 16.84%\n",
      "Loss: 1.30 Accuracy 16.87%\n",
      "Loss: 1.30 Accuracy 16.87%\n",
      "Loss: 1.31 Accuracy 16.86%\n",
      "Loss: 1.29 Accuracy 16.87%\n",
      "Loss: 1.32 Accuracy 16.87%\n",
      "Loss: 1.32 Accuracy 16.86%\n",
      "Loss 1.32 Test Accuracy: 55.77%\n",
      "Epoch 3\n",
      "Loss: 0.13 Accuracy 21.88%\n",
      "Loss: 0.86 Accuracy 19.32%\n",
      "Loss: 1.12 Accuracy 19.05%\n",
      "Loss: 1.19 Accuracy 19.35%\n",
      "Loss: 1.24 Accuracy 19.36%\n",
      "Loss: 1.22 Accuracy 19.61%\n",
      "Loss: 1.23 Accuracy 19.72%\n",
      "Loss: 1.24 Accuracy 19.81%\n",
      "Loss: 1.22 Accuracy 19.87%\n",
      "Loss: 1.26 Accuracy 19.40%\n",
      "Loss: 1.24 Accuracy 19.55%\n",
      "Loss: 1.22 Accuracy 19.57%\n",
      "Loss: 1.23 Accuracy 19.71%\n",
      "Loss: 1.25 Accuracy 19.49%\n",
      "Loss: 1.25 Accuracy 19.39%\n",
      "Loss: 1.27 Accuracy 19.27%\n",
      "Loss: 1.23 Accuracy 19.37%\n",
      "Loss: 1.22 Accuracy 19.54%\n",
      "Loss: 1.22 Accuracy 19.61%\n",
      "Loss: 1.24 Accuracy 19.54%\n",
      "Loss: 1.21 Accuracy 19.71%\n",
      "Loss: 1.22 Accuracy 19.71%\n",
      "Loss: 1.22 Accuracy 19.74%\n",
      "Loss: 1.22 Accuracy 19.81%\n",
      "Loss: 1.22 Accuracy 19.88%\n",
      "Loss: 1.24 Accuracy 19.88%\n",
      "Loss: 1.22 Accuracy 19.91%\n",
      "Loss: 1.25 Accuracy 19.87%\n",
      "Loss: 1.21 Accuracy 19.94%\n",
      "Loss: 1.24 Accuracy 19.92%\n",
      "Loss: 1.21 Accuracy 19.91%\n",
      "Loss: 1.22 Accuracy 19.96%\n",
      "Loss: 1.22 Accuracy 19.98%\n",
      "Loss: 1.26 Accuracy 19.84%\n",
      "Loss: 1.24 Accuracy 19.81%\n",
      "Loss: 1.23 Accuracy 19.80%\n",
      "Loss: 1.24 Accuracy 19.74%\n",
      "Loss: 1.25 Accuracy 19.68%\n",
      "Loss: 1.21 Accuracy 19.76%\n",
      "Loss: 1.22 Accuracy 19.78%\n",
      "Loss: 1.23 Accuracy 19.79%\n",
      "Loss: 1.21 Accuracy 19.82%\n",
      "Loss: 1.24 Accuracy 19.83%\n",
      "Loss: 1.23 Accuracy 19.80%\n",
      "Loss: 1.23 Accuracy 19.79%\n",
      "Loss: 1.25 Accuracy 19.80%\n",
      "Loss: 1.27 Accuracy 19.73%\n",
      "Loss: 1.24 Accuracy 19.74%\n",
      "Loss: 1.25 Accuracy 19.69%\n",
      "Loss: 1.26 Accuracy 19.62%\n",
      "Loss: 1.23 Accuracy 19.65%\n",
      "Loss: 1.26 Accuracy 19.60%\n",
      "Loss: 1.20 Accuracy 19.69%\n",
      "Loss: 1.22 Accuracy 19.73%\n",
      "Loss: 1.18 Accuracy 19.82%\n",
      "Loss: 1.20 Accuracy 19.86%\n",
      "Loss: 1.25 Accuracy 19.81%\n",
      "Loss: 1.26 Accuracy 19.73%\n",
      "Loss: 1.23 Accuracy 19.72%\n",
      "Loss: 1.20 Accuracy 19.77%\n",
      "Loss: 1.25 Accuracy 19.71%\n",
      "Loss: 1.22 Accuracy 19.73%\n",
      "Loss: 1.23 Accuracy 19.71%\n",
      "Loss: 1.26 Accuracy 19.68%\n",
      "Loss: 1.26 Accuracy 19.66%\n",
      "Loss: 1.24 Accuracy 19.68%\n",
      "Loss: 1.25 Accuracy 19.67%\n",
      "Loss: 1.26 Accuracy 19.67%\n",
      "Loss: 1.24 Accuracy 19.65%\n",
      "Loss: 1.24 Accuracy 19.66%\n",
      "Loss: 1.22 Accuracy 19.70%\n",
      "Loss: 1.21 Accuracy 19.70%\n",
      "Loss: 1.23 Accuracy 19.71%\n",
      "Loss: 1.20 Accuracy 19.75%\n",
      "Loss: 1.23 Accuracy 19.73%\n",
      "Loss: 1.24 Accuracy 19.71%\n",
      "Loss: 1.28 Accuracy 19.69%\n",
      "Loss: 1.29 Accuracy 19.65%\n",
      "Loss: 1.27 Accuracy 19.61%\n",
      "Loss: 1.24 Accuracy 19.63%\n",
      "Loss: 1.29 Accuracy 19.60%\n",
      "Loss: 1.29 Accuracy 19.58%\n",
      "Loss: 1.26 Accuracy 19.58%\n",
      "Loss: 1.24 Accuracy 19.60%\n",
      "Loss: 1.29 Accuracy 19.57%\n",
      "Loss: 1.26 Accuracy 19.58%\n",
      "Loss: 1.24 Accuracy 19.59%\n",
      "Loss: 1.26 Accuracy 19.60%\n",
      "Loss: 1.22 Accuracy 19.62%\n",
      "Loss: 1.23 Accuracy 19.63%\n",
      "Loss: 1.27 Accuracy 19.58%\n",
      "Loss: 1.24 Accuracy 19.59%\n",
      "Loss: 1.26 Accuracy 19.61%\n",
      "Loss: 1.24 Accuracy 19.62%\n",
      "Loss: 1.21 Accuracy 19.67%\n",
      "Loss: 1.15 Accuracy 19.76%\n",
      "Loss: 1.20 Accuracy 19.78%\n",
      "Loss: 1.22 Accuracy 19.77%\n",
      "Loss: 1.19 Accuracy 19.83%\n",
      "Loss: 1.23 Accuracy 19.82%\n",
      "Loss: 1.22 Accuracy 19.83%\n",
      "Loss: 1.25 Accuracy 19.82%\n",
      "Loss: 1.25 Accuracy 19.81%\n",
      "Loss: 1.26 Accuracy 19.79%\n",
      "Loss: 1.25 Accuracy 19.79%\n",
      "Loss: 1.25 Accuracy 19.78%\n",
      "Loss: 1.26 Accuracy 19.77%\n",
      "Loss: 1.25 Accuracy 19.77%\n",
      "Loss: 1.25 Accuracy 19.75%\n",
      "Loss: 1.24 Accuracy 19.74%\n",
      "Loss: 1.22 Accuracy 19.76%\n",
      "Loss: 1.20 Accuracy 19.79%\n",
      "Loss: 1.20 Accuracy 19.82%\n",
      "Loss: 1.23 Accuracy 19.81%\n",
      "Loss: 1.25 Accuracy 19.80%\n",
      "Loss: 1.24 Accuracy 19.79%\n",
      "Loss: 1.24 Accuracy 19.79%\n",
      "Loss: 1.26 Accuracy 19.77%\n",
      "Loss: 1.27 Accuracy 19.75%\n",
      "Loss: 1.27 Accuracy 19.73%\n",
      "Loss: 1.23 Accuracy 19.75%\n",
      "Loss: 1.28 Accuracy 19.73%\n",
      "Loss: 1.24 Accuracy 19.74%\n",
      "Loss: 1.25 Accuracy 19.74%\n",
      "Loss: 1.25 Accuracy 19.73%\n",
      "Loss: 1.20 Accuracy 19.76%\n",
      "Loss: 1.27 Accuracy 19.75%\n",
      "Loss: 1.26 Accuracy 19.74%\n",
      "Loss: 1.26 Accuracy 19.72%\n",
      "Loss: 1.29 Accuracy 19.71%\n",
      "Loss: 1.22 Accuracy 19.73%\n",
      "Loss: 1.23 Accuracy 19.73%\n",
      "Loss: 1.23 Accuracy 19.73%\n",
      "Loss: 1.20 Accuracy 19.75%\n",
      "Loss: 1.22 Accuracy 19.75%\n",
      "Loss: 1.25 Accuracy 19.75%\n",
      "Loss: 1.23 Accuracy 19.76%\n",
      "Loss: 1.23 Accuracy 19.76%\n",
      "Loss: 1.23 Accuracy 19.76%\n",
      "Loss: 1.26 Accuracy 19.77%\n",
      "Loss: 1.25 Accuracy 19.77%\n",
      "Loss: 1.23 Accuracy 19.78%\n",
      "Loss: 1.20 Accuracy 19.80%\n",
      "Loss: 1.20 Accuracy 19.81%\n",
      "Loss: 1.22 Accuracy 19.82%\n",
      "Loss: 1.22 Accuracy 19.84%\n",
      "Loss: 1.19 Accuracy 19.86%\n",
      "Loss: 1.18 Accuracy 19.89%\n",
      "Loss: 1.26 Accuracy 19.87%\n",
      "Loss: 1.27 Accuracy 19.87%\n",
      "Loss: 1.28 Accuracy 19.83%\n",
      "Loss 1.28 Test Accuracy: 58.58%\n",
      "Epoch 4\n",
      "Loss: 0.11 Accuracy 25.00%\n",
      "Loss: 0.85 Accuracy 18.47%\n",
      "Loss: 1.09 Accuracy 18.45%\n",
      "Loss: 1.15 Accuracy 19.86%\n",
      "Loss: 1.17 Accuracy 19.97%\n",
      "Loss: 1.17 Accuracy 20.47%\n",
      "Loss: 1.17 Accuracy 20.59%\n",
      "Loss: 1.15 Accuracy 20.99%\n",
      "Loss: 1.21 Accuracy 20.87%\n",
      "Loss: 1.18 Accuracy 21.22%\n",
      "Loss: 1.17 Accuracy 21.35%\n",
      "Loss: 1.21 Accuracy 21.14%\n",
      "Loss: 1.20 Accuracy 21.02%\n",
      "Loss: 1.20 Accuracy 21.02%\n",
      "Loss: 1.23 Accuracy 20.77%\n",
      "Loss: 1.25 Accuracy 20.57%\n",
      "Loss: 1.21 Accuracy 20.61%\n",
      "Loss: 1.18 Accuracy 20.69%\n",
      "Loss: 1.18 Accuracy 20.75%\n",
      "Loss: 1.16 Accuracy 20.84%\n",
      "Loss: 1.15 Accuracy 21.00%\n",
      "Loss: 1.18 Accuracy 21.12%\n",
      "Loss: 1.18 Accuracy 21.10%\n",
      "Loss: 1.21 Accuracy 21.04%\n",
      "Loss: 1.20 Accuracy 21.02%\n",
      "Loss: 1.21 Accuracy 21.02%\n",
      "Loss: 1.22 Accuracy 20.98%\n",
      "Loss: 1.21 Accuracy 20.93%\n",
      "Loss: 1.19 Accuracy 20.91%\n",
      "Loss: 1.20 Accuracy 20.91%\n",
      "Loss: 1.20 Accuracy 20.88%\n",
      "Loss: 1.16 Accuracy 21.02%\n",
      "Loss: 1.16 Accuracy 21.09%\n",
      "Loss: 1.18 Accuracy 21.07%\n",
      "Loss: 1.17 Accuracy 21.12%\n",
      "Loss: 1.21 Accuracy 21.06%\n",
      "Loss: 1.21 Accuracy 21.01%\n",
      "Loss: 1.22 Accuracy 20.98%\n",
      "Loss: 1.21 Accuracy 20.97%\n",
      "Loss: 1.18 Accuracy 21.03%\n",
      "Loss: 1.20 Accuracy 20.99%\n",
      "Loss: 1.19 Accuracy 21.03%\n",
      "Loss: 1.17 Accuracy 21.10%\n",
      "Loss: 1.14 Accuracy 21.20%\n",
      "Loss: 1.18 Accuracy 21.22%\n",
      "Loss: 1.16 Accuracy 21.27%\n",
      "Loss: 1.19 Accuracy 21.27%\n",
      "Loss: 1.17 Accuracy 21.35%\n",
      "Loss: 1.17 Accuracy 21.39%\n",
      "Loss: 1.19 Accuracy 21.38%\n",
      "Loss: 1.20 Accuracy 21.37%\n",
      "Loss: 1.20 Accuracy 21.39%\n",
      "Loss: 1.17 Accuracy 21.46%\n",
      "Loss: 1.18 Accuracy 21.46%\n",
      "Loss: 1.19 Accuracy 21.44%\n",
      "Loss: 1.19 Accuracy 21.44%\n",
      "Loss: 1.14 Accuracy 21.55%\n",
      "Loss: 1.18 Accuracy 21.55%\n",
      "Loss: 1.22 Accuracy 21.47%\n",
      "Loss: 1.19 Accuracy 21.50%\n",
      "Loss: 1.17 Accuracy 21.54%\n",
      "Loss: 1.20 Accuracy 21.50%\n",
      "Loss: 1.18 Accuracy 21.50%\n",
      "Loss: 1.16 Accuracy 21.55%\n",
      "Loss: 1.20 Accuracy 21.51%\n",
      "Loss: 1.19 Accuracy 21.50%\n",
      "Loss: 1.19 Accuracy 21.49%\n",
      "Loss: 1.18 Accuracy 21.50%\n",
      "Loss: 1.22 Accuracy 21.42%\n",
      "Loss: 1.20 Accuracy 21.42%\n",
      "Loss: 1.21 Accuracy 21.39%\n",
      "Loss: 1.17 Accuracy 21.44%\n",
      "Loss: 1.21 Accuracy 21.44%\n",
      "Loss: 1.17 Accuracy 21.49%\n",
      "Loss: 1.16 Accuracy 21.52%\n",
      "Loss: 1.19 Accuracy 21.50%\n",
      "Loss: 1.16 Accuracy 21.51%\n",
      "Loss: 1.20 Accuracy 21.51%\n",
      "Loss: 1.18 Accuracy 21.54%\n",
      "Loss: 1.19 Accuracy 21.56%\n",
      "Loss: 1.19 Accuracy 21.57%\n",
      "Loss: 1.15 Accuracy 21.60%\n",
      "Loss: 1.19 Accuracy 21.57%\n",
      "Loss: 1.16 Accuracy 21.63%\n",
      "Loss: 1.21 Accuracy 21.56%\n",
      "Loss: 1.18 Accuracy 21.56%\n",
      "Loss: 1.17 Accuracy 21.57%\n",
      "Loss: 1.14 Accuracy 21.61%\n",
      "Loss: 1.16 Accuracy 21.60%\n",
      "Loss: 1.15 Accuracy 21.63%\n",
      "Loss: 1.18 Accuracy 21.63%\n",
      "Loss: 1.19 Accuracy 21.59%\n",
      "Loss: 1.19 Accuracy 21.60%\n",
      "Loss: 1.18 Accuracy 21.61%\n",
      "Loss: 1.19 Accuracy 21.61%\n",
      "Loss: 1.24 Accuracy 21.56%\n",
      "Loss: 1.20 Accuracy 21.55%\n",
      "Loss: 1.17 Accuracy 21.56%\n",
      "Loss: 1.16 Accuracy 21.58%\n",
      "Loss: 1.22 Accuracy 21.53%\n",
      "Loss: 1.22 Accuracy 21.51%\n",
      "Loss: 1.20 Accuracy 21.49%\n",
      "Loss: 1.19 Accuracy 21.49%\n",
      "Loss: 1.22 Accuracy 21.44%\n",
      "Loss: 1.21 Accuracy 21.43%\n",
      "Loss: 1.21 Accuracy 21.44%\n",
      "Loss: 1.22 Accuracy 21.42%\n",
      "Loss: 1.22 Accuracy 21.40%\n",
      "Loss: 1.22 Accuracy 21.38%\n",
      "Loss: 1.17 Accuracy 21.40%\n",
      "Loss: 1.15 Accuracy 21.42%\n",
      "Loss: 1.19 Accuracy 21.39%\n",
      "Loss: 1.17 Accuracy 21.41%\n",
      "Loss: 1.18 Accuracy 21.41%\n",
      "Loss: 1.20 Accuracy 21.41%\n",
      "Loss: 1.18 Accuracy 21.41%\n",
      "Loss: 1.19 Accuracy 21.40%\n",
      "Loss: 1.19 Accuracy 21.40%\n",
      "Loss: 1.19 Accuracy 21.42%\n",
      "Loss: 1.20 Accuracy 21.41%\n",
      "Loss: 1.22 Accuracy 21.39%\n",
      "Loss: 1.20 Accuracy 21.39%\n",
      "Loss: 1.17 Accuracy 21.40%\n",
      "Loss: 1.18 Accuracy 21.39%\n",
      "Loss: 1.20 Accuracy 21.37%\n",
      "Loss: 1.21 Accuracy 21.35%\n",
      "Loss: 1.17 Accuracy 21.39%\n",
      "Loss: 1.16 Accuracy 21.40%\n",
      "Loss: 1.19 Accuracy 21.40%\n",
      "Loss: 1.18 Accuracy 21.42%\n",
      "Loss: 1.16 Accuracy 21.43%\n",
      "Loss: 1.18 Accuracy 21.44%\n",
      "Loss: 1.19 Accuracy 21.44%\n",
      "Loss: 1.21 Accuracy 21.42%\n",
      "Loss: 1.18 Accuracy 21.43%\n",
      "Loss: 1.23 Accuracy 21.38%\n",
      "Loss: 1.20 Accuracy 21.38%\n",
      "Loss: 1.16 Accuracy 21.40%\n",
      "Loss: 1.21 Accuracy 21.37%\n",
      "Loss: 1.24 Accuracy 21.34%\n",
      "Loss: 1.22 Accuracy 21.34%\n",
      "Loss: 1.23 Accuracy 21.33%\n",
      "Loss: 1.21 Accuracy 21.35%\n",
      "Loss: 1.18 Accuracy 21.36%\n",
      "Loss: 1.20 Accuracy 21.35%\n",
      "Loss: 1.20 Accuracy 21.35%\n",
      "Loss: 1.20 Accuracy 21.34%\n",
      "Loss: 1.17 Accuracy 21.35%\n",
      "Loss: 1.21 Accuracy 21.33%\n",
      "Loss: 1.20 Accuracy 21.31%\n",
      "Loss: 1.17 Accuracy 21.31%\n",
      "Loss 1.17 Test Accuracy: 59.86%\n",
      "Epoch 5\n",
      "Loss: 0.11 Accuracy 28.12%\n",
      "Loss: 0.79 Accuracy 25.57%\n",
      "Loss: 1.03 Accuracy 23.81%\n",
      "Loss: 1.13 Accuracy 22.78%\n",
      "Loss: 1.16 Accuracy 22.64%\n",
      "Loss: 1.16 Accuracy 22.43%\n",
      "Loss: 1.18 Accuracy 22.23%\n",
      "Loss: 1.18 Accuracy 22.05%\n",
      "Loss: 1.20 Accuracy 21.91%\n",
      "Loss: 1.19 Accuracy 21.91%\n",
      "Loss: 1.12 Accuracy 22.18%\n",
      "Loss: 1.15 Accuracy 22.04%\n",
      "Loss: 1.15 Accuracy 22.08%\n",
      "Loss: 1.14 Accuracy 22.30%\n",
      "Loss: 1.13 Accuracy 22.47%\n",
      "Loss: 1.17 Accuracy 22.35%\n",
      "Loss: 1.15 Accuracy 22.53%\n",
      "Loss: 1.17 Accuracy 22.46%\n",
      "Loss: 1.17 Accuracy 22.46%\n",
      "Loss: 1.17 Accuracy 22.32%\n",
      "Loss: 1.15 Accuracy 22.42%\n",
      "Loss: 1.16 Accuracy 22.38%\n",
      "Loss: 1.19 Accuracy 22.27%\n",
      "Loss: 1.21 Accuracy 22.13%\n",
      "Loss: 1.20 Accuracy 22.04%\n",
      "Loss: 1.19 Accuracy 21.94%\n",
      "Loss: 1.15 Accuracy 22.02%\n",
      "Loss: 1.20 Accuracy 21.85%\n",
      "Loss: 1.16 Accuracy 21.85%\n",
      "Loss: 1.20 Accuracy 21.72%\n",
      "Loss: 1.16 Accuracy 21.78%\n",
      "Loss: 1.14 Accuracy 21.83%\n",
      "Loss: 1.17 Accuracy 21.86%\n",
      "Loss: 1.16 Accuracy 21.87%\n",
      "Loss: 1.19 Accuracy 21.77%\n",
      "Loss: 1.17 Accuracy 21.79%\n",
      "Loss: 1.15 Accuracy 21.82%\n",
      "Loss: 1.17 Accuracy 21.77%\n",
      "Loss: 1.16 Accuracy 21.80%\n",
      "Loss: 1.13 Accuracy 21.89%\n",
      "Loss: 1.20 Accuracy 21.80%\n",
      "Loss: 1.16 Accuracy 21.83%\n",
      "Loss: 1.17 Accuracy 21.79%\n",
      "Loss: 1.21 Accuracy 21.70%\n",
      "Loss: 1.20 Accuracy 21.66%\n",
      "Loss: 1.18 Accuracy 21.65%\n",
      "Loss: 1.19 Accuracy 21.58%\n",
      "Loss: 1.19 Accuracy 21.57%\n",
      "Loss: 1.20 Accuracy 21.54%\n",
      "Loss: 1.19 Accuracy 21.55%\n",
      "Loss: 1.20 Accuracy 21.52%\n",
      "Loss: 1.18 Accuracy 21.51%\n",
      "Loss: 1.18 Accuracy 21.53%\n",
      "Loss: 1.24 Accuracy 21.42%\n",
      "Loss: 1.18 Accuracy 21.41%\n",
      "Loss: 1.19 Accuracy 21.39%\n",
      "Loss: 1.19 Accuracy 21.36%\n",
      "Loss: 1.18 Accuracy 21.38%\n",
      "Loss: 1.20 Accuracy 21.35%\n",
      "Loss: 1.20 Accuracy 21.33%\n",
      "Loss: 1.18 Accuracy 21.33%\n",
      "Loss: 1.14 Accuracy 21.39%\n",
      "Loss: 1.15 Accuracy 21.41%\n",
      "Loss: 1.18 Accuracy 21.40%\n",
      "Loss: 1.14 Accuracy 21.46%\n",
      "Loss: 1.19 Accuracy 21.43%\n",
      "Loss: 1.16 Accuracy 21.48%\n",
      "Loss: 1.17 Accuracy 21.45%\n",
      "Loss: 1.12 Accuracy 21.54%\n",
      "Loss: 1.17 Accuracy 21.52%\n",
      "Loss: 1.16 Accuracy 21.55%\n",
      "Loss: 1.18 Accuracy 21.53%\n",
      "Loss: 1.18 Accuracy 21.52%\n",
      "Loss: 1.20 Accuracy 21.49%\n",
      "Loss: 1.17 Accuracy 21.52%\n",
      "Loss: 1.16 Accuracy 21.57%\n",
      "Loss: 1.16 Accuracy 21.57%\n",
      "Loss: 1.16 Accuracy 21.58%\n",
      "Loss: 1.15 Accuracy 21.61%\n",
      "Loss: 1.13 Accuracy 21.65%\n",
      "Loss: 1.14 Accuracy 21.68%\n",
      "Loss: 1.15 Accuracy 21.70%\n",
      "Loss: 1.15 Accuracy 21.70%\n",
      "Loss: 1.17 Accuracy 21.70%\n",
      "Loss: 1.15 Accuracy 21.71%\n",
      "Loss: 1.17 Accuracy 21.70%\n",
      "Loss: 1.23 Accuracy 21.62%\n",
      "Loss: 1.15 Accuracy 21.68%\n",
      "Loss: 1.16 Accuracy 21.69%\n",
      "Loss: 1.18 Accuracy 21.71%\n",
      "Loss: 1.14 Accuracy 21.75%\n",
      "Loss: 1.17 Accuracy 21.71%\n",
      "Loss: 1.17 Accuracy 21.71%\n",
      "Loss: 1.15 Accuracy 21.73%\n",
      "Loss: 1.16 Accuracy 21.74%\n",
      "Loss: 1.12 Accuracy 21.81%\n",
      "Loss: 1.15 Accuracy 21.79%\n",
      "Loss: 1.16 Accuracy 21.79%\n",
      "Loss: 1.18 Accuracy 21.78%\n",
      "Loss: 1.20 Accuracy 21.75%\n",
      "Loss: 1.15 Accuracy 21.78%\n",
      "Loss: 1.19 Accuracy 21.75%\n",
      "Loss: 1.18 Accuracy 21.75%\n",
      "Loss: 1.16 Accuracy 21.76%\n",
      "Loss: 1.14 Accuracy 21.79%\n",
      "Loss: 1.13 Accuracy 21.82%\n",
      "Loss: 1.16 Accuracy 21.82%\n",
      "Loss: 1.16 Accuracy 21.83%\n",
      "Loss: 1.18 Accuracy 21.81%\n",
      "Loss: 1.19 Accuracy 21.80%\n",
      "Loss: 1.13 Accuracy 21.86%\n",
      "Loss: 1.17 Accuracy 21.85%\n",
      "Loss: 1.14 Accuracy 21.88%\n",
      "Loss: 1.21 Accuracy 21.84%\n",
      "Loss: 1.16 Accuracy 21.85%\n",
      "Loss: 1.20 Accuracy 21.83%\n",
      "Loss: 1.18 Accuracy 21.84%\n",
      "Loss: 1.17 Accuracy 21.83%\n",
      "Loss: 1.13 Accuracy 21.86%\n",
      "Loss: 1.16 Accuracy 21.86%\n",
      "Loss: 1.12 Accuracy 21.89%\n",
      "Loss: 1.15 Accuracy 21.89%\n",
      "Loss: 1.17 Accuracy 21.89%\n",
      "Loss: 1.20 Accuracy 21.86%\n",
      "Loss: 1.18 Accuracy 21.85%\n",
      "Loss: 1.19 Accuracy 21.84%\n",
      "Loss: 1.16 Accuracy 21.87%\n",
      "Loss: 1.12 Accuracy 21.89%\n",
      "Loss: 1.15 Accuracy 21.90%\n",
      "Loss: 1.19 Accuracy 21.89%\n",
      "Loss: 1.21 Accuracy 21.87%\n",
      "Loss: 1.21 Accuracy 21.85%\n",
      "Loss: 1.23 Accuracy 21.83%\n",
      "Loss: 1.17 Accuracy 21.84%\n",
      "Loss: 1.14 Accuracy 21.88%\n",
      "Loss: 1.14 Accuracy 21.90%\n",
      "Loss: 1.12 Accuracy 21.93%\n",
      "Loss: 1.13 Accuracy 21.94%\n",
      "Loss: 1.16 Accuracy 21.93%\n",
      "Loss: 1.20 Accuracy 21.92%\n",
      "Loss: 1.16 Accuracy 21.94%\n",
      "Loss: 1.16 Accuracy 21.93%\n",
      "Loss: 1.17 Accuracy 21.93%\n",
      "Loss: 1.10 Accuracy 21.96%\n",
      "Loss: 1.15 Accuracy 21.97%\n",
      "Loss: 1.13 Accuracy 22.00%\n",
      "Loss: 1.14 Accuracy 22.01%\n",
      "Loss: 1.19 Accuracy 21.99%\n",
      "Loss: 1.15 Accuracy 22.00%\n",
      "Loss: 1.14 Accuracy 22.03%\n",
      "Loss: 1.15 Accuracy 22.04%\n",
      "Loss 1.15 Test Accuracy: 58.10%\n",
      "Epoch 6\n",
      "Loss: 0.10 Accuracy 34.38%\n",
      "Loss: 0.80 Accuracy 21.31%\n",
      "Loss: 1.03 Accuracy 22.17%\n",
      "Loss: 1.10 Accuracy 22.48%\n",
      "Loss: 1.17 Accuracy 21.95%\n",
      "Loss: 1.14 Accuracy 22.49%\n",
      "Loss: 1.19 Accuracy 21.82%\n",
      "Loss: 1.17 Accuracy 21.96%\n",
      "Loss: 1.16 Accuracy 22.30%\n",
      "Loss: 1.19 Accuracy 21.98%\n",
      "Loss: 1.19 Accuracy 22.03%\n",
      "Loss: 1.17 Accuracy 22.13%\n",
      "Loss: 1.16 Accuracy 22.08%\n",
      "Loss: 1.14 Accuracy 22.28%\n",
      "Loss: 1.14 Accuracy 22.45%\n",
      "Loss: 1.19 Accuracy 22.14%\n",
      "Loss: 1.20 Accuracy 21.93%\n",
      "Loss: 1.20 Accuracy 21.89%\n",
      "Loss: 1.15 Accuracy 22.01%\n",
      "Loss: 1.16 Accuracy 22.02%\n",
      "Loss: 1.16 Accuracy 22.05%\n",
      "Loss: 1.16 Accuracy 22.07%\n",
      "Loss: 1.15 Accuracy 22.07%\n",
      "Loss: 1.18 Accuracy 22.00%\n",
      "Loss: 1.17 Accuracy 22.02%\n",
      "Loss: 1.20 Accuracy 21.94%\n",
      "Loss: 1.15 Accuracy 22.05%\n",
      "Loss: 1.11 Accuracy 22.24%\n",
      "Loss: 1.10 Accuracy 22.39%\n",
      "Loss: 1.12 Accuracy 22.43%\n",
      "Loss: 1.12 Accuracy 22.47%\n",
      "Loss: 1.14 Accuracy 22.53%\n",
      "Loss: 1.17 Accuracy 22.48%\n",
      "Loss: 1.17 Accuracy 22.43%\n",
      "Loss: 1.14 Accuracy 22.42%\n",
      "Loss: 1.19 Accuracy 22.32%\n",
      "Loss: 1.18 Accuracy 22.25%\n",
      "Loss: 1.15 Accuracy 22.30%\n",
      "Loss: 1.12 Accuracy 22.38%\n",
      "Loss: 1.12 Accuracy 22.40%\n",
      "Loss: 1.10 Accuracy 22.51%\n",
      "Loss: 1.10 Accuracy 22.60%\n",
      "Loss: 1.14 Accuracy 22.59%\n",
      "Loss: 1.13 Accuracy 22.60%\n",
      "Loss: 1.18 Accuracy 22.51%\n",
      "Loss: 1.15 Accuracy 22.53%\n",
      "Loss: 1.19 Accuracy 22.42%\n",
      "Loss: 1.16 Accuracy 22.43%\n",
      "Loss: 1.15 Accuracy 22.47%\n",
      "Loss: 1.15 Accuracy 22.44%\n",
      "Loss: 1.11 Accuracy 22.54%\n",
      "Loss: 1.12 Accuracy 22.55%\n",
      "Loss: 1.13 Accuracy 22.58%\n",
      "Loss: 1.17 Accuracy 22.52%\n",
      "Loss: 1.16 Accuracy 22.52%\n",
      "Loss: 1.14 Accuracy 22.54%\n",
      "Loss: 1.14 Accuracy 22.54%\n",
      "Loss: 1.08 Accuracy 22.65%\n",
      "Loss: 1.13 Accuracy 22.66%\n",
      "Loss: 1.14 Accuracy 22.66%\n",
      "Loss: 1.14 Accuracy 22.68%\n",
      "Loss: 1.14 Accuracy 22.68%\n",
      "Loss: 1.11 Accuracy 22.73%\n",
      "Loss: 1.12 Accuracy 22.75%\n",
      "Loss: 1.13 Accuracy 22.76%\n",
      "Loss: 1.13 Accuracy 22.76%\n",
      "Loss: 1.17 Accuracy 22.72%\n",
      "Loss: 1.21 Accuracy 22.65%\n",
      "Loss: 1.21 Accuracy 22.60%\n",
      "Loss: 1.22 Accuracy 22.52%\n",
      "Loss: 1.17 Accuracy 22.53%\n",
      "Loss: 1.12 Accuracy 22.56%\n",
      "Loss: 1.16 Accuracy 22.52%\n",
      "Loss: 1.13 Accuracy 22.56%\n",
      "Loss: 1.14 Accuracy 22.57%\n",
      "Loss: 1.16 Accuracy 22.54%\n",
      "Loss: 1.15 Accuracy 22.55%\n",
      "Loss: 1.14 Accuracy 22.56%\n",
      "Loss: 1.10 Accuracy 22.61%\n",
      "Loss: 1.14 Accuracy 22.61%\n",
      "Loss: 1.15 Accuracy 22.60%\n",
      "Loss: 1.14 Accuracy 22.62%\n",
      "Loss: 1.18 Accuracy 22.58%\n",
      "Loss: 1.17 Accuracy 22.57%\n",
      "Loss: 1.15 Accuracy 22.56%\n",
      "Loss: 1.11 Accuracy 22.61%\n",
      "Loss: 1.10 Accuracy 22.63%\n",
      "Loss: 1.10 Accuracy 22.66%\n",
      "Loss: 1.13 Accuracy 22.67%\n",
      "Loss: 1.20 Accuracy 22.61%\n",
      "Loss: 1.17 Accuracy 22.61%\n",
      "Loss: 1.15 Accuracy 22.62%\n",
      "Loss: 1.14 Accuracy 22.62%\n",
      "Loss: 1.15 Accuracy 22.63%\n",
      "Loss: 1.18 Accuracy 22.60%\n",
      "Loss: 1.21 Accuracy 22.54%\n",
      "Loss: 1.16 Accuracy 22.55%\n",
      "Loss: 1.18 Accuracy 22.53%\n",
      "Loss: 1.18 Accuracy 22.50%\n",
      "Loss: 1.14 Accuracy 22.51%\n",
      "Loss: 1.17 Accuracy 22.50%\n",
      "Loss: 1.21 Accuracy 22.44%\n",
      "Loss: 1.19 Accuracy 22.44%\n",
      "Loss: 1.16 Accuracy 22.44%\n",
      "Loss: 1.18 Accuracy 22.41%\n",
      "Loss: 1.19 Accuracy 22.38%\n",
      "Loss: 1.14 Accuracy 22.41%\n",
      "Loss: 1.14 Accuracy 22.41%\n",
      "Loss: 1.12 Accuracy 22.44%\n",
      "Loss: 1.16 Accuracy 22.42%\n",
      "Loss: 1.19 Accuracy 22.40%\n",
      "Loss: 1.14 Accuracy 22.41%\n",
      "Loss: 1.19 Accuracy 22.39%\n",
      "Loss: 1.17 Accuracy 22.40%\n",
      "Loss: 1.14 Accuracy 22.41%\n",
      "Loss: 1.15 Accuracy 22.41%\n",
      "Loss: 1.13 Accuracy 22.41%\n",
      "Loss: 1.16 Accuracy 22.41%\n",
      "Loss: 1.15 Accuracy 22.40%\n",
      "Loss: 1.17 Accuracy 22.41%\n",
      "Loss: 1.16 Accuracy 22.41%\n",
      "Loss: 1.20 Accuracy 22.37%\n",
      "Loss: 1.15 Accuracy 22.38%\n",
      "Loss: 1.18 Accuracy 22.36%\n",
      "Loss: 1.11 Accuracy 22.40%\n",
      "Loss: 1.13 Accuracy 22.40%\n",
      "Loss: 1.13 Accuracy 22.40%\n",
      "Loss: 1.15 Accuracy 22.40%\n",
      "Loss: 1.12 Accuracy 22.43%\n",
      "Loss: 1.13 Accuracy 22.44%\n",
      "Loss: 1.18 Accuracy 22.43%\n",
      "Loss: 1.17 Accuracy 22.41%\n",
      "Loss: 1.16 Accuracy 22.41%\n",
      "Loss: 1.16 Accuracy 22.41%\n",
      "Loss: 1.16 Accuracy 22.41%\n",
      "Loss: 1.17 Accuracy 22.39%\n",
      "Loss: 1.23 Accuracy 22.34%\n",
      "Loss: 1.20 Accuracy 22.34%\n",
      "Loss: 1.21 Accuracy 22.31%\n",
      "Loss: 1.17 Accuracy 22.31%\n",
      "Loss: 1.19 Accuracy 22.29%\n",
      "Loss: 1.17 Accuracy 22.29%\n",
      "Loss: 1.17 Accuracy 22.28%\n",
      "Loss: 1.20 Accuracy 22.26%\n",
      "Loss: 1.13 Accuracy 22.28%\n",
      "Loss: 1.12 Accuracy 22.29%\n",
      "Loss: 1.12 Accuracy 22.33%\n",
      "Loss: 1.16 Accuracy 22.31%\n",
      "Loss: 1.21 Accuracy 22.28%\n",
      "Loss: 1.20 Accuracy 22.28%\n",
      "Loss: 1.15 Accuracy 22.29%\n",
      "Loss 1.15 Test Accuracy: 58.83%\n",
      "Epoch 7\n",
      "Loss: 0.11 Accuracy 25.00%\n",
      "Loss: 0.77 Accuracy 23.01%\n",
      "Loss: 1.02 Accuracy 23.07%\n",
      "Loss: 1.09 Accuracy 22.88%\n",
      "Loss: 1.14 Accuracy 22.41%\n",
      "Loss: 1.14 Accuracy 22.73%\n",
      "Loss: 1.16 Accuracy 22.69%\n",
      "Loss: 1.17 Accuracy 22.58%\n",
      "Loss: 1.15 Accuracy 22.61%\n",
      "Loss: 1.16 Accuracy 22.49%\n",
      "Loss: 1.18 Accuracy 22.22%\n",
      "Loss: 1.18 Accuracy 22.16%\n",
      "Loss: 1.17 Accuracy 22.11%\n",
      "Loss: 1.17 Accuracy 22.09%\n",
      "Loss: 1.20 Accuracy 21.88%\n",
      "Loss: 1.20 Accuracy 21.81%\n",
      "Loss: 1.19 Accuracy 21.84%\n",
      "Loss: 1.19 Accuracy 21.71%\n",
      "Loss: 1.18 Accuracy 21.77%\n",
      "Loss: 1.19 Accuracy 21.71%\n",
      "Loss: 1.19 Accuracy 21.64%\n",
      "Loss: 1.18 Accuracy 21.62%\n",
      "Loss: 1.19 Accuracy 21.56%\n",
      "Loss: 1.18 Accuracy 21.58%\n",
      "Loss: 1.17 Accuracy 21.52%\n",
      "Loss: 1.16 Accuracy 21.55%\n",
      "Loss: 1.12 Accuracy 21.67%\n",
      "Loss: 1.14 Accuracy 21.73%\n",
      "Loss: 1.16 Accuracy 21.75%\n",
      "Loss: 1.14 Accuracy 21.75%\n",
      "Loss: 1.14 Accuracy 21.74%\n",
      "Loss: 1.14 Accuracy 21.80%\n",
      "Loss: 1.12 Accuracy 21.89%\n",
      "Loss: 1.17 Accuracy 21.81%\n",
      "Loss: 1.16 Accuracy 21.84%\n",
      "Loss: 1.17 Accuracy 21.79%\n",
      "Loss: 1.18 Accuracy 21.74%\n",
      "Loss: 1.18 Accuracy 21.77%\n",
      "Loss: 1.14 Accuracy 21.84%\n",
      "Loss: 1.13 Accuracy 21.85%\n",
      "Loss: 1.18 Accuracy 21.82%\n",
      "Loss: 1.21 Accuracy 21.72%\n",
      "Loss: 1.12 Accuracy 21.85%\n",
      "Loss: 1.12 Accuracy 21.93%\n",
      "Loss: 1.15 Accuracy 21.94%\n",
      "Loss: 1.18 Accuracy 21.94%\n",
      "Loss: 1.17 Accuracy 21.93%\n",
      "Loss: 1.15 Accuracy 21.95%\n",
      "Loss: 1.15 Accuracy 21.95%\n",
      "Loss: 1.13 Accuracy 22.03%\n",
      "Loss: 1.14 Accuracy 22.05%\n",
      "Loss: 1.16 Accuracy 22.05%\n",
      "Loss: 1.22 Accuracy 21.95%\n",
      "Loss: 1.20 Accuracy 21.90%\n",
      "Loss: 1.20 Accuracy 21.85%\n",
      "Loss: 1.19 Accuracy 21.81%\n",
      "Loss: 1.15 Accuracy 21.84%\n",
      "Loss: 1.17 Accuracy 21.86%\n",
      "Loss: 1.14 Accuracy 21.91%\n",
      "Loss: 1.15 Accuracy 21.95%\n",
      "Loss: 1.16 Accuracy 21.96%\n",
      "Loss: 1.15 Accuracy 21.96%\n",
      "Loss: 1.16 Accuracy 21.97%\n",
      "Loss: 1.17 Accuracy 21.97%\n",
      "Loss: 1.14 Accuracy 22.00%\n",
      "Loss: 1.13 Accuracy 22.03%\n",
      "Loss: 1.13 Accuracy 22.06%\n",
      "Loss: 1.15 Accuracy 22.07%\n",
      "Loss: 1.18 Accuracy 22.03%\n",
      "Loss: 1.18 Accuracy 21.99%\n",
      "Loss: 1.16 Accuracy 21.98%\n",
      "Loss: 1.19 Accuracy 21.96%\n",
      "Loss: 1.20 Accuracy 21.92%\n",
      "Loss: 1.17 Accuracy 21.93%\n",
      "Loss: 1.16 Accuracy 21.92%\n",
      "Loss: 1.14 Accuracy 21.97%\n",
      "Loss: 1.16 Accuracy 21.97%\n",
      "Loss: 1.14 Accuracy 22.00%\n",
      "Loss: 1.14 Accuracy 22.02%\n",
      "Loss: 1.14 Accuracy 22.04%\n",
      "Loss: 1.18 Accuracy 22.01%\n",
      "Loss: 1.16 Accuracy 22.02%\n",
      "Loss: 1.15 Accuracy 22.05%\n",
      "Loss: 1.16 Accuracy 22.04%\n",
      "Loss: 1.17 Accuracy 22.04%\n",
      "Loss: 1.16 Accuracy 22.06%\n",
      "Loss: 1.15 Accuracy 22.07%\n",
      "Loss: 1.11 Accuracy 22.10%\n",
      "Loss: 1.15 Accuracy 22.08%\n",
      "Loss: 1.15 Accuracy 22.09%\n",
      "Loss: 1.18 Accuracy 22.08%\n",
      "Loss: 1.16 Accuracy 22.08%\n",
      "Loss: 1.16 Accuracy 22.07%\n",
      "Loss: 1.15 Accuracy 22.09%\n",
      "Loss: 1.13 Accuracy 22.12%\n",
      "Loss: 1.13 Accuracy 22.12%\n",
      "Loss: 1.14 Accuracy 22.14%\n",
      "Loss: 1.17 Accuracy 22.13%\n",
      "Loss: 1.20 Accuracy 22.09%\n",
      "Loss: 1.15 Accuracy 22.10%\n",
      "Loss: 1.17 Accuracy 22.09%\n",
      "Loss: 1.14 Accuracy 22.11%\n",
      "Loss: 1.12 Accuracy 22.13%\n",
      "Loss: 1.15 Accuracy 22.12%\n",
      "Loss: 1.17 Accuracy 22.12%\n",
      "Loss: 1.12 Accuracy 22.16%\n",
      "Loss: 1.14 Accuracy 22.16%\n",
      "Loss: 1.16 Accuracy 22.15%\n",
      "Loss: 1.16 Accuracy 22.16%\n",
      "Loss: 1.11 Accuracy 22.20%\n",
      "Loss: 1.17 Accuracy 22.17%\n",
      "Loss: 1.19 Accuracy 22.13%\n",
      "Loss: 1.18 Accuracy 22.13%\n",
      "Loss: 1.15 Accuracy 22.15%\n",
      "Loss: 1.13 Accuracy 22.15%\n",
      "Loss: 1.09 Accuracy 22.19%\n",
      "Loss: 1.12 Accuracy 22.20%\n",
      "Loss: 1.13 Accuracy 22.20%\n",
      "Loss: 1.13 Accuracy 22.22%\n",
      "Loss: 1.16 Accuracy 22.20%\n",
      "Loss: 1.11 Accuracy 22.23%\n",
      "Loss: 1.15 Accuracy 22.23%\n",
      "Loss: 1.16 Accuracy 22.22%\n",
      "Loss: 1.15 Accuracy 22.22%\n",
      "Loss: 1.17 Accuracy 22.21%\n",
      "Loss: 1.15 Accuracy 22.22%\n",
      "Loss: 1.20 Accuracy 22.19%\n",
      "Loss: 1.18 Accuracy 22.19%\n",
      "Loss: 1.18 Accuracy 22.18%\n",
      "Loss: 1.16 Accuracy 22.18%\n",
      "Loss: 1.16 Accuracy 22.19%\n",
      "Loss: 1.13 Accuracy 22.21%\n",
      "Loss: 1.13 Accuracy 22.21%\n",
      "Loss: 1.15 Accuracy 22.20%\n",
      "Loss: 1.21 Accuracy 22.16%\n",
      "Loss: 1.15 Accuracy 22.19%\n",
      "Loss: 1.19 Accuracy 22.16%\n",
      "Loss: 1.18 Accuracy 22.14%\n",
      "Loss: 1.20 Accuracy 22.12%\n",
      "Loss: 1.19 Accuracy 22.11%\n",
      "Loss: 1.16 Accuracy 22.12%\n",
      "Loss: 1.14 Accuracy 22.12%\n",
      "Loss: 1.17 Accuracy 22.11%\n",
      "Loss: 1.14 Accuracy 22.12%\n",
      "Loss: 1.18 Accuracy 22.09%\n",
      "Loss: 1.13 Accuracy 22.11%\n",
      "Loss: 1.14 Accuracy 22.13%\n",
      "Loss: 1.12 Accuracy 22.15%\n",
      "Loss: 1.15 Accuracy 22.14%\n",
      "Loss: 1.18 Accuracy 22.13%\n",
      "Loss: 1.19 Accuracy 22.13%\n",
      "Loss 1.19 Test Accuracy: 58.33%\n",
      "Epoch 8\n",
      "Loss: 0.11 Accuracy 25.00%\n",
      "Loss: 0.80 Accuracy 22.16%\n",
      "Loss: 1.03 Accuracy 23.21%\n",
      "Loss: 1.13 Accuracy 22.58%\n",
      "Loss: 1.17 Accuracy 22.33%\n",
      "Loss: 1.12 Accuracy 23.22%\n",
      "Loss: 1.12 Accuracy 23.51%\n",
      "Loss: 1.14 Accuracy 23.42%\n",
      "Loss: 1.17 Accuracy 23.07%\n",
      "Loss: 1.16 Accuracy 22.94%\n",
      "Loss: 1.12 Accuracy 23.14%\n",
      "Loss: 1.16 Accuracy 22.78%\n",
      "Loss: 1.12 Accuracy 23.09%\n",
      "Loss: 1.13 Accuracy 22.92%\n",
      "Loss: 1.16 Accuracy 22.74%\n",
      "Loss: 1.14 Accuracy 22.74%\n",
      "Loss: 1.15 Accuracy 22.71%\n",
      "Loss: 1.12 Accuracy 22.93%\n",
      "Loss: 1.15 Accuracy 22.82%\n",
      "Loss: 1.17 Accuracy 22.63%\n",
      "Loss: 1.14 Accuracy 22.75%\n",
      "Loss: 1.19 Accuracy 22.53%\n",
      "Loss: 1.16 Accuracy 22.53%\n",
      "Loss: 1.15 Accuracy 22.55%\n",
      "Loss: 1.12 Accuracy 22.68%\n",
      "Loss: 1.16 Accuracy 22.53%\n",
      "Loss: 1.13 Accuracy 22.62%\n",
      "Loss: 1.17 Accuracy 22.51%\n",
      "Loss: 1.13 Accuracy 22.59%\n",
      "Loss: 1.15 Accuracy 22.56%\n",
      "Loss: 1.17 Accuracy 22.50%\n",
      "Loss: 1.12 Accuracy 22.58%\n",
      "Loss: 1.11 Accuracy 22.63%\n",
      "Loss: 1.16 Accuracy 22.56%\n",
      "Loss: 1.16 Accuracy 22.53%\n",
      "Loss: 1.16 Accuracy 22.57%\n",
      "Loss: 1.13 Accuracy 22.60%\n",
      "Loss: 1.16 Accuracy 22.50%\n",
      "Loss: 1.13 Accuracy 22.50%\n",
      "Loss: 1.13 Accuracy 22.51%\n",
      "Loss: 1.12 Accuracy 22.58%\n",
      "Loss: 1.18 Accuracy 22.49%\n",
      "Loss: 1.17 Accuracy 22.49%\n",
      "Loss: 1.14 Accuracy 22.54%\n",
      "Loss: 1.16 Accuracy 22.53%\n",
      "Loss: 1.13 Accuracy 22.60%\n",
      "Loss: 1.17 Accuracy 22.52%\n",
      "Loss: 1.16 Accuracy 22.54%\n",
      "Loss: 1.12 Accuracy 22.62%\n",
      "Loss: 1.14 Accuracy 22.63%\n",
      "Loss: 1.15 Accuracy 22.62%\n",
      "Loss: 1.14 Accuracy 22.63%\n",
      "Loss: 1.14 Accuracy 22.67%\n",
      "Loss: 1.11 Accuracy 22.72%\n",
      "Loss: 1.15 Accuracy 22.70%\n",
      "Loss: 1.17 Accuracy 22.66%\n",
      "Loss: 1.16 Accuracy 22.63%\n",
      "Loss: 1.21 Accuracy 22.54%\n",
      "Loss: 1.17 Accuracy 22.53%\n",
      "Loss: 1.14 Accuracy 22.54%\n",
      "Loss: 1.17 Accuracy 22.50%\n",
      "Loss: 1.11 Accuracy 22.57%\n",
      "Loss: 1.11 Accuracy 22.62%\n",
      "Loss: 1.12 Accuracy 22.65%\n",
      "Loss: 1.13 Accuracy 22.68%\n",
      "Loss: 1.15 Accuracy 22.65%\n",
      "Loss: 1.13 Accuracy 22.68%\n",
      "Loss: 1.13 Accuracy 22.70%\n",
      "Loss: 1.17 Accuracy 22.70%\n",
      "Loss: 1.16 Accuracy 22.65%\n",
      "Loss: 1.14 Accuracy 22.68%\n",
      "Loss: 1.11 Accuracy 22.73%\n",
      "Loss: 1.16 Accuracy 22.72%\n",
      "Loss: 1.17 Accuracy 22.69%\n",
      "Loss: 1.14 Accuracy 22.71%\n",
      "Loss: 1.16 Accuracy 22.68%\n",
      "Loss: 1.17 Accuracy 22.69%\n",
      "Loss: 1.15 Accuracy 22.70%\n",
      "Loss: 1.13 Accuracy 22.74%\n",
      "Loss: 1.15 Accuracy 22.73%\n",
      "Loss: 1.18 Accuracy 22.70%\n",
      "Loss: 1.17 Accuracy 22.70%\n",
      "Loss: 1.14 Accuracy 22.70%\n",
      "Loss: 1.18 Accuracy 22.67%\n",
      "Loss: 1.20 Accuracy 22.63%\n",
      "Loss: 1.16 Accuracy 22.64%\n",
      "Loss: 1.13 Accuracy 22.66%\n",
      "Loss: 1.18 Accuracy 22.61%\n",
      "Loss: 1.12 Accuracy 22.63%\n",
      "Loss: 1.12 Accuracy 22.66%\n",
      "Loss: 1.09 Accuracy 22.71%\n",
      "Loss: 1.16 Accuracy 22.67%\n",
      "Loss: 1.19 Accuracy 22.66%\n",
      "Loss: 1.21 Accuracy 22.61%\n",
      "Loss: 1.17 Accuracy 22.61%\n",
      "Loss: 1.15 Accuracy 22.61%\n",
      "Loss: 1.14 Accuracy 22.62%\n",
      "Loss: 1.16 Accuracy 22.60%\n",
      "Loss: 1.16 Accuracy 22.60%\n",
      "Loss: 1.19 Accuracy 22.57%\n",
      "Loss: 1.17 Accuracy 22.56%\n",
      "Loss: 1.17 Accuracy 22.54%\n",
      "Loss: 1.17 Accuracy 22.54%\n",
      "Loss: 1.18 Accuracy 22.52%\n",
      "Loss: 1.16 Accuracy 22.51%\n",
      "Loss: 1.16 Accuracy 22.50%\n",
      "Loss: 1.14 Accuracy 22.51%\n",
      "Loss: 1.12 Accuracy 22.55%\n",
      "Loss: 1.13 Accuracy 22.55%\n",
      "Loss: 1.14 Accuracy 22.55%\n",
      "Loss: 1.13 Accuracy 22.55%\n",
      "Loss: 1.16 Accuracy 22.55%\n",
      "Loss: 1.19 Accuracy 22.52%\n",
      "Loss: 1.20 Accuracy 22.50%\n",
      "Loss: 1.16 Accuracy 22.51%\n",
      "Loss: 1.18 Accuracy 22.48%\n",
      "Loss: 1.20 Accuracy 22.46%\n",
      "Loss: 1.25 Accuracy 22.41%\n",
      "Loss: 1.21 Accuracy 22.40%\n",
      "Loss: 1.18 Accuracy 22.39%\n",
      "Loss: 1.20 Accuracy 22.37%\n",
      "Loss: 1.19 Accuracy 22.35%\n",
      "Loss: 1.15 Accuracy 22.36%\n",
      "Loss: 1.17 Accuracy 22.34%\n",
      "Loss: 1.15 Accuracy 22.35%\n",
      "Loss: 1.15 Accuracy 22.35%\n",
      "Loss: 1.16 Accuracy 22.35%\n",
      "Loss: 1.16 Accuracy 22.36%\n",
      "Loss: 1.18 Accuracy 22.35%\n",
      "Loss: 1.17 Accuracy 22.34%\n",
      "Loss: 1.15 Accuracy 22.36%\n",
      "Loss: 1.17 Accuracy 22.35%\n",
      "Loss: 1.19 Accuracy 22.34%\n",
      "Loss: 1.17 Accuracy 22.34%\n",
      "Loss: 1.19 Accuracy 22.32%\n",
      "Loss: 1.18 Accuracy 22.31%\n",
      "Loss: 1.11 Accuracy 22.34%\n",
      "Loss: 1.18 Accuracy 22.32%\n",
      "Loss: 1.15 Accuracy 22.34%\n",
      "Loss: 1.12 Accuracy 22.35%\n",
      "Loss: 1.17 Accuracy 22.34%\n",
      "Loss: 1.15 Accuracy 22.35%\n",
      "Loss: 1.16 Accuracy 22.35%\n",
      "Loss: 1.18 Accuracy 22.33%\n",
      "Loss: 1.17 Accuracy 22.33%\n",
      "Loss: 1.14 Accuracy 22.34%\n",
      "Loss: 1.16 Accuracy 22.33%\n",
      "Loss: 1.17 Accuracy 22.32%\n",
      "Loss: 1.19 Accuracy 22.31%\n",
      "Loss: 1.14 Accuracy 22.32%\n",
      "Loss: 1.12 Accuracy 22.32%\n",
      "Loss 1.12 Test Accuracy: 59.54%\n",
      "Epoch 9\n",
      "Loss: 0.11 Accuracy 28.12%\n",
      "Loss: 0.80 Accuracy 22.44%\n",
      "Loss: 1.00 Accuracy 23.96%\n",
      "Loss: 1.09 Accuracy 23.59%\n",
      "Loss: 1.19 Accuracy 22.18%\n",
      "Loss: 1.17 Accuracy 22.00%\n",
      "Loss: 1.11 Accuracy 22.80%\n",
      "Loss: 1.17 Accuracy 22.58%\n",
      "Loss: 1.15 Accuracy 22.76%\n",
      "Loss: 1.17 Accuracy 22.49%\n",
      "Loss: 1.16 Accuracy 22.56%\n",
      "Loss: 1.18 Accuracy 22.47%\n",
      "Loss: 1.16 Accuracy 22.37%\n",
      "Loss: 1.18 Accuracy 22.21%\n",
      "Loss: 1.14 Accuracy 22.41%\n",
      "Loss: 1.12 Accuracy 22.66%\n",
      "Loss: 1.18 Accuracy 22.46%\n",
      "Loss: 1.19 Accuracy 22.22%\n",
      "Loss: 1.12 Accuracy 22.51%\n",
      "Loss: 1.11 Accuracy 22.71%\n",
      "Loss: 1.16 Accuracy 22.70%\n",
      "Loss: 1.17 Accuracy 22.57%\n",
      "Loss: 1.13 Accuracy 22.68%\n",
      "Loss: 1.13 Accuracy 22.70%\n",
      "Loss: 1.12 Accuracy 22.78%\n",
      "Loss: 1.13 Accuracy 22.87%\n",
      "Loss: 1.17 Accuracy 22.80%\n",
      "Loss: 1.16 Accuracy 22.79%\n",
      "Loss: 1.17 Accuracy 22.71%\n",
      "Loss: 1.19 Accuracy 22.65%\n",
      "Loss: 1.14 Accuracy 22.72%\n",
      "Loss: 1.19 Accuracy 22.59%\n",
      "Loss: 1.18 Accuracy 22.58%\n",
      "Loss: 1.14 Accuracy 22.59%\n",
      "Loss: 1.15 Accuracy 22.56%\n",
      "Loss: 1.13 Accuracy 22.62%\n",
      "Loss: 1.17 Accuracy 22.55%\n",
      "Loss: 1.15 Accuracy 22.55%\n",
      "Loss: 1.17 Accuracy 22.50%\n",
      "Loss: 1.16 Accuracy 22.48%\n",
      "Loss: 1.18 Accuracy 22.44%\n",
      "Loss: 1.18 Accuracy 22.39%\n",
      "Loss: 1.17 Accuracy 22.36%\n",
      "Loss: 1.18 Accuracy 22.30%\n",
      "Loss: 1.17 Accuracy 22.25%\n",
      "Loss: 1.14 Accuracy 22.28%\n",
      "Loss: 1.14 Accuracy 22.32%\n",
      "Loss: 1.16 Accuracy 22.27%\n",
      "Loss: 1.17 Accuracy 22.20%\n",
      "Loss: 1.12 Accuracy 22.27%\n",
      "Loss: 1.13 Accuracy 22.32%\n",
      "Loss: 1.15 Accuracy 22.35%\n",
      "Loss: 1.13 Accuracy 22.36%\n",
      "Loss: 1.16 Accuracy 22.35%\n",
      "Loss: 1.15 Accuracy 22.36%\n",
      "Loss: 1.15 Accuracy 22.35%\n",
      "Loss: 1.12 Accuracy 22.38%\n",
      "Loss: 1.15 Accuracy 22.37%\n",
      "Loss: 1.13 Accuracy 22.39%\n",
      "Loss: 1.17 Accuracy 22.33%\n",
      "Loss: 1.18 Accuracy 22.31%\n",
      "Loss: 1.20 Accuracy 22.26%\n",
      "Loss: 1.18 Accuracy 22.26%\n",
      "Loss: 1.16 Accuracy 22.27%\n",
      "Loss: 1.16 Accuracy 22.29%\n",
      "Loss: 1.15 Accuracy 22.29%\n",
      "Loss: 1.13 Accuracy 22.32%\n",
      "Loss: 1.13 Accuracy 22.36%\n",
      "Loss: 1.16 Accuracy 22.32%\n",
      "Loss: 1.20 Accuracy 22.25%\n",
      "Loss: 1.16 Accuracy 22.27%\n",
      "Loss: 1.18 Accuracy 22.26%\n",
      "Loss: 1.13 Accuracy 22.30%\n",
      "Loss: 1.14 Accuracy 22.30%\n",
      "Loss: 1.12 Accuracy 22.35%\n",
      "Loss: 1.15 Accuracy 22.36%\n",
      "Loss: 1.14 Accuracy 22.36%\n",
      "Loss: 1.13 Accuracy 22.37%\n",
      "Loss: 1.18 Accuracy 22.34%\n",
      "Loss: 1.17 Accuracy 22.33%\n",
      "Loss: 1.13 Accuracy 22.37%\n",
      "Loss: 1.18 Accuracy 22.34%\n",
      "Loss: 1.21 Accuracy 22.28%\n",
      "Loss: 1.18 Accuracy 22.29%\n",
      "Loss: 1.18 Accuracy 22.28%\n",
      "Loss: 1.17 Accuracy 22.27%\n",
      "Loss: 1.12 Accuracy 22.33%\n",
      "Loss: 1.13 Accuracy 22.34%\n",
      "Loss: 1.16 Accuracy 22.31%\n",
      "Loss: 1.14 Accuracy 22.32%\n",
      "Loss: 1.14 Accuracy 22.33%\n",
      "Loss: 1.19 Accuracy 22.30%\n",
      "Loss: 1.15 Accuracy 22.32%\n",
      "Loss: 1.15 Accuracy 22.33%\n",
      "Loss: 1.11 Accuracy 22.38%\n",
      "Loss: 1.15 Accuracy 22.38%\n",
      "Loss: 1.13 Accuracy 22.41%\n",
      "Loss: 1.15 Accuracy 22.41%\n",
      "Loss: 1.15 Accuracy 22.40%\n",
      "Loss: 1.15 Accuracy 22.39%\n",
      "Loss: 1.16 Accuracy 22.39%\n",
      "Loss: 1.16 Accuracy 22.39%\n",
      "Loss: 1.14 Accuracy 22.40%\n",
      "Loss: 1.17 Accuracy 22.38%\n",
      "Loss: 1.17 Accuracy 22.38%\n",
      "Loss: 1.15 Accuracy 22.39%\n",
      "Loss: 1.14 Accuracy 22.41%\n",
      "Loss: 1.12 Accuracy 22.42%\n",
      "Loss: 1.06 Accuracy 22.49%\n",
      "Loss: 1.12 Accuracy 22.49%\n",
      "Loss: 1.16 Accuracy 22.46%\n",
      "Loss: 1.16 Accuracy 22.48%\n",
      "Loss: 1.14 Accuracy 22.51%\n",
      "Loss: 1.16 Accuracy 22.49%\n",
      "Loss: 1.16 Accuracy 22.49%\n",
      "Loss: 1.18 Accuracy 22.47%\n",
      "Loss: 1.16 Accuracy 22.48%\n",
      "Loss: 1.16 Accuracy 22.47%\n",
      "Loss: 1.22 Accuracy 22.44%\n",
      "Loss: 1.16 Accuracy 22.45%\n",
      "Loss: 1.14 Accuracy 22.46%\n",
      "Loss: 1.18 Accuracy 22.42%\n",
      "Loss: 1.15 Accuracy 22.42%\n",
      "Loss: 1.19 Accuracy 22.39%\n",
      "Loss: 1.16 Accuracy 22.39%\n",
      "Loss: 1.15 Accuracy 22.40%\n",
      "Loss: 1.18 Accuracy 22.39%\n",
      "Loss: 1.16 Accuracy 22.39%\n",
      "Loss: 1.19 Accuracy 22.36%\n",
      "Loss: 1.16 Accuracy 22.36%\n",
      "Loss: 1.17 Accuracy 22.35%\n",
      "Loss: 1.17 Accuracy 22.34%\n",
      "Loss: 1.16 Accuracy 22.34%\n",
      "Loss: 1.16 Accuracy 22.34%\n",
      "Loss: 1.18 Accuracy 22.32%\n",
      "Loss: 1.16 Accuracy 22.33%\n",
      "Loss: 1.18 Accuracy 22.31%\n",
      "Loss: 1.13 Accuracy 22.33%\n",
      "Loss: 1.12 Accuracy 22.34%\n",
      "Loss: 1.13 Accuracy 22.36%\n",
      "Loss: 1.14 Accuracy 22.36%\n",
      "Loss: 1.12 Accuracy 22.38%\n",
      "Loss: 1.11 Accuracy 22.40%\n",
      "Loss: 1.13 Accuracy 22.42%\n",
      "Loss: 1.20 Accuracy 22.39%\n",
      "Loss: 1.24 Accuracy 22.35%\n",
      "Loss: 1.22 Accuracy 22.32%\n",
      "Loss: 1.19 Accuracy 22.32%\n",
      "Loss: 1.17 Accuracy 22.32%\n",
      "Loss: 1.18 Accuracy 22.30%\n",
      "Loss: 1.17 Accuracy 22.30%\n",
      "Loss 1.17 Test Accuracy: 58.65%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "\n",
    "model = GRUClassifier(num_classes=20, n_vocab=len(vocab))\n",
    "model.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs)\n",
    "\n",
    "def prep(x):\n",
    "    return torch.tensor([vocab[v] for v in tokenizer(x)], dtype=torch.int64)\n",
    "\n",
    "train_in_dataset = NewsGroup20(\"data/\", train=True, transform=prep)\n",
    "test_dataset = NewsGroup20(\"data/\", train=False, transform=prep)\n",
    "\n",
    "train_out_dataset = WikiText2(\"data/\", split=\"train\", transform=prep, target_transform=ToUnknown()) \n",
    "\n",
    "train_loader = DataLoader(train_in_dataset + train_out_dataset, batch_size=32, shuffle=True, \n",
    "                          collate_fn=collate_batch)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "\n",
    "model.train()\n",
    "model.cuda()\n",
    "\n",
    "criterion = OutlierExposureLoss(alpha=0.5)\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    loss_ema = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    model.train()\n",
    "    for n, batch in enumerate(train_loader):\n",
    "        inputs, labels = batch\n",
    "\n",
    "        inputs = inputs.cuda()\n",
    "        labels = labels.cuda()\n",
    "        logits = model(inputs)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_ema = loss_ema * 0.9 + loss.data.cpu().numpy() * 0.1\n",
    "\n",
    "        pred = logits.max(dim=1).indices\n",
    "        correct += pred[labels >= 0].eq(labels[labels >= 0]).sum().data.cpu().numpy()\n",
    "        total += pred[labels >= 0].shape[0]\n",
    "\n",
    "        if n % 10 == 0:\n",
    "            print(f\"Loss: {loss_ema.item():.2f} Accuracy {correct/total:.2%}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for n, batch in enumerate(test_loader):\n",
    "            inputs, labels = batch\n",
    "\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "            logits = model(inputs)\n",
    "            pred = logits.max(dim=1).indices\n",
    "            correct += pred.eq(labels).sum().data.cpu().numpy()\n",
    "            total += pred.shape[0]\n",
    "\n",
    "        print(f\"Loss {loss_ema:.2f} Test Accuracy: {correct/total:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dfs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3727157/28938317.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# df[\"Temperature\"] = temp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mdfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dfs' is not defined"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "\n",
    "ood_dataset = Reuters52(\"data/\", train=False, download=True, transform=prep, target_transform=ToUnknown())\n",
    "res+= test(model, test_dataset + ood_dataset, dataset_name=\"Reuters52\")\n",
    "\n",
    "ood_dataset = Multi30k(\"data/\", train=False, download=True, transform=prep, target_transform=ToUnknown())\n",
    "res+= test(model, test_dataset + ood_dataset, dataset_name=\"Multi30k\")\n",
    "\n",
    "ood_dataset = WMT16Sentences(\"data/\", download=True, transform=prep, target_transform=ToUnknown())\n",
    "res+= test(model, test_dataset + ood_dataset, dataset_name=\"WMT16Sentences\")\n",
    "\n",
    "df = pd.DataFrame(res)\n",
    "# df[\"Temperature\"] = temp\n",
    "# dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrr}\n",
      "\\toprule\n",
      "{} &  AUROC &  AUPR-IN &  AUPR-OUT &  ACC95TPR &  FPR95TPR \\\\\n",
      "Method  &        &          &           &           &           \\\\\n",
      "\\midrule\n",
      "Energy  &  94.41 &    86.23 &     97.87 &     85.59 &     16.85 \\\\\n",
      "Softmax &  93.84 &    86.34 &     97.56 &     83.59 &     19.55 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "df = pd.DataFrame(res)\n",
    "print((df.groupby(\"Method\").mean() * 100).to_latex(float_format=\"%.2f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls /data_slow/kirchheim/gan_oe/text-generation/work_language_model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
